----------Potential Bugs:

In env.py----------
The calc_total_latency() function reintializes in the same list, instead of appending
self.start_time = self.arrive_time in class Task(objects)

In greedy.py----------
return np.random.choice(m_cpu[1]) generates a random sample from the range 0 to the server of lowest cpu

In opt.py----------
result = v.varName[2] gives the second char defined in addVar(name="x_" + str(i)), so result='_' not int


----------Notes:
this is implmeented already
    # Set up optimizers for policy and q-function
    def setup_optimizers(self):
        self.pi_optimizer = torch.optim.Adam(self.policy.actor.parameters(), lr=lr_actor)
        self.q_optimizer = torch.optim.Adam(self.q_params, lr=lr_critic)

uncomment and work out the clamping function:            #a2 = torch.clamp(a2, -act_limit, act_limit)

change TD3 annonate style depending on the convergence point in reward plot

